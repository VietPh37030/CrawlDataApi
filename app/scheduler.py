"""
Scheduler - Auto crawl má»—i 15 phÃºt + Realtime Log
"""
import asyncio
from datetime import datetime, timezone
from typing import Optional, List
from collections import deque

class CrawlScheduler:
    def __init__(self):
        self.is_running = False
        self.auto_enabled = False
        self.interval_minutes = 15
        self.last_run: Optional[datetime] = None
        self.task: Optional[asyncio.Task] = None
        
        # Realtime tracking
        self.current_story = ""
        self.current_story_title = ""
        self.crawl_logs: deque = deque(maxlen=20)  # Last 20 logs
        self.stats = {
            "stories_crawled": 0,
            "chapters_saved": 0,
            "errors": 0,
        }
        
        # Progress tracking for current story
        self.progress = {
            "current_chapter": 0,
            "total_chapters": 0,
            "percent": 0,
            "status": "idle",  # idle, crawling_list, crawling_story, saving_chapters, done
        }
        
    def _log(self, message: str):
        """Add log entry"""
        entry = {
            "time": datetime.now().strftime("%H:%M:%S"),
            "message": message
        }
        self.crawl_logs.append(entry)
        print(f"[{entry['time']}] {message}")
        
    async def start_auto_crawl(self):
        """Báº­t auto-crawl"""
        if self.auto_enabled:
            return {"status": "already_running"}
        
        self.auto_enabled = True
        self.task = asyncio.create_task(self._auto_crawl_loop())
        self._log("ğŸ”„ Auto-crawl Ä‘Ã£ Báº¬T")
        return {"status": "started", "interval": self.interval_minutes}
    
    def stop_auto_crawl(self):
        """Táº¯t auto-crawl VÃ€ dá»«ng manual crawl"""
        self.auto_enabled = False
        self.is_running = False
        if self.task:
            self.task.cancel()
            self.task = None
        self.current_story = ""
        self._log("â¹ï¸ Crawler Ä‘Ã£ Dá»ªNG")
        return {"status": "stopped"}
    
    async def _auto_crawl_loop(self):
        """Loop cháº¡y auto crawl"""
        while self.auto_enabled:
            try:
                self._log("â° Auto-crawl báº¯t Ä‘áº§u!")
                await self._run_crawl_job()
                self.last_run = datetime.now()
                self._log(f"âœ… Auto-crawl xong! Äá»£i {self.interval_minutes} phÃºt...")
                await asyncio.sleep(self.interval_minutes * 60)
            except asyncio.CancelledError:
                break
            except Exception as e:
                self._log(f"âŒ Lá»—i: {e}")
                await asyncio.sleep(60)
    
    async def _run_crawl_job(self):
        """Cháº¡y crawl job"""
        from .crawler.crawler import StoryCrawler
        from .database import Database
        
        crawler = StoryCrawler()
        db = Database()
        
        self._log("ğŸ“š Äang láº¥y danh sÃ¡ch truyá»‡n má»›i...")
        try:
            stories = await crawler.crawl_story_list(
                f"{crawler.settings.base_url}/danh-sach/truyen-moi/",
                max_pages=2
            )
            self._log(f"ğŸ“‹ TÃ¬m tháº¥y {len(stories)} truyá»‡n")
            
            for story_info in stories[:10]:
                if not self.auto_enabled:
                    break
                try:
                    await self._crawl_and_save_story(crawler, db, story_info["source_url"])
                except Exception as e:
                    self.stats["errors"] += 1
                    continue
                    
        except Exception as e:
            self._log(f"âŒ Lá»—i crawl: {e}")
    
    async def _crawl_and_save_story(self, crawler, db, url: str):
        """Crawl vÃ  lÆ°u 1 truyá»‡n + Táº¤T Cáº¢ chapters trÆ°á»›c khi sang truyá»‡n khÃ¡c"""
        # Extract title from URL for display
        slug = url.rstrip('/').split('/')[-1]
        self.current_story = slug
        self.progress["status"] = "crawling_story"
        self.progress["current_chapter"] = 0
        self.progress["total_chapters"] = 0
        self.progress["percent"] = 0
        self._log(f"ğŸ“– Báº¯t Ä‘áº§u crawl: {slug}")
        
        try:
            # Crawl story VÃ€ láº¥y danh sÃ¡ch chapters tá»« Táº¤T Cáº¢ pages
            # include_chapters=False nghÄ©a lÃ  khÃ´ng crawl Ná»˜I DUNG chapter (cháº­m)
            # nhÆ°ng VáºªN láº¥y DANH SÃCH chapters (title, source_url, chapter_number)
            story = await crawler.crawl_story(url, include_chapters=False)
            
            raw_chapters = story.get("chapters", [])
            
            # Khá»­ trÃ¹ng láº·p chapter_number trÆ°á»›c khi lÆ°u
            # VÃ¬ Postgres khÃ´ng cho phÃ©p cÃ³ 2 dÃ²ng cÃ¹ng unique key trong 1 lá»‡nh UPSERT batch
            seen_chapters = {}
            for ch in raw_chapters:
                ch_num = ch.get("chapter_number")
                if ch_num not in seen_chapters:
                    seen_chapters[ch_num] = ch
            
            chapters = list(seen_chapters.values())
            # Sáº¯p xáº¿p láº¡i theo sá»‘ chÆ°Æ¡ng
            chapters.sort(key=lambda x: x.get("chapter_number", 0))
            
            total_chapters = len(chapters)
            self.progress["total_chapters"] = total_chapters
            self._log(f"  ğŸ“‹ TÃ¬m tháº¥y {len(raw_chapters)} chÆ°Æ¡ng (Khá»­ trÃ¹ng cÃ²n {total_chapters})")
            
            if total_chapters == 0:
                self._log(f"  âš ï¸ KhÃ´ng tÃ¬m tháº¥y chapter nÃ o, bá» qua")
                return
            
            # LÆ°u story trÆ°á»›c
            story_record = {
                "slug": story["slug"],
                "title": story["title"],
                "author": story.get("author"),
                "description": story.get("description"),
                "genres": story.get("genres", []),
                "status": "Full" if story.get("status") == "completed" else "Äang ra",
                "total_chapters": total_chapters,
                "cover_url": story.get("cover_url"),
                "source_url": story.get("source_url"),
                "updated_at": datetime.now(timezone.utc).isoformat(),
            }
            
            saved_story = await db.upsert_story(story_record)
            story_id = saved_story.get("id") if saved_story else None
            
            if not story_id:
                existing = await db.get_story_by_slug(story["slug"])
                story_id = existing["id"] if existing else None
            
            if not story_id:
                self._log(f"  âŒ KhÃ´ng lÆ°u Ä‘Æ°á»£c truyá»‡n: {story['title']}")
                self.stats["errors"] += 1
                return
            
            self.current_story_title = story['title']
            self.progress["status"] = "saving_chapters"
            self._log(f"  âœ… ÄÃ£ lÆ°u truyá»‡n: {story['title'][:40]}")
            
            # LÆ°u Táº¤T Cáº¢ chapters theo batch Ä‘á»ƒ tá»‘i Æ°u
            saved_count = 0
            batch_size = 50
            
            for i in range(0, total_chapters, batch_size):
                if not self.is_running and not self.auto_enabled:
                    self._log(f"  â¹ï¸ ÄÃ£ dá»«ng giá»¯a chá»«ng")
                    break
                    
                batch = chapters[i:i + batch_size]
                chapter_records = []
                
                for ch in batch:
                    chapter_records.append({
                        "story_id": story_id,
                        "chapter_number": ch.get("chapter_number", 0),
                        "title": ch.get("title", ""),
                        "source_url": ch.get("source_url", ""),
                        "content": "",  # Content sáº½ crawl sau náº¿u cáº§n
                    })
                
                try:
                    await db.bulk_upsert_chapters(chapter_records)
                    saved_count += len(chapter_records)
                    
                    # Log tiáº¿n trÃ¬nh + cáº­p nháº­t progress
                    progress = min(i + batch_size, total_chapters)
                    self.progress["current_chapter"] = progress
                    self.progress["percent"] = int((progress / total_chapters) * 100) if total_chapters > 0 else 0
                    self._log(f"  ğŸ“ ÄÃ£ lÆ°u {progress}/{total_chapters} chapters ({self.progress['percent']}%)")
                except Exception as e:
                    self._log(f"  âš ï¸ Lá»—i batch {i}: {e}")
                    # Fallback: lÆ°u tá»«ng chapter
                    for ch_rec in chapter_records:
                        try:
                            await db.upsert_chapter(ch_rec)
                            saved_count += 1
                        except Exception:
                            continue
            
            self.stats["stories_crawled"] += 1
            self.stats["chapters_saved"] += saved_count
            self.progress["status"] = "done"
            self.progress["percent"] = 100
            self._log(f"  ğŸ‰ HoÃ n thÃ nh: {story['title'][:30]}... ({saved_count}/{total_chapters} chÆ°Æ¡ng)")
            
            # Cáº­p nháº­t thá»‘ng kÃª vÃ o database Ä‘á»ƒ charts hiá»ƒn thá»‹
            try:
                await db.update_crawl_stats(stories=1, chapters=saved_count)
            except Exception as stats_error:
                self._log(f"  âš ï¸ Lá»—i cáº­p nháº­t stats: {stats_error}")
            
        except Exception as e:
            self._log(f"  âŒ Lá»—i crawl {slug}: {e}")
            self.stats["errors"] += 1
    
    async def manual_crawl(self, categories: list, max_pages: int):
        """Crawl thá»§ cÃ´ng"""
        if self.is_running:
            return {"status": "busy", "message": "Crawler Ä‘ang cháº¡y"}
        
        self.is_running = True
        self.stats = {"stories_crawled": 0, "chapters_saved": 0, "errors": 0}
        self._log(f"ğŸš€ Báº¯t Ä‘áº§u crawl: {categories}")
        
        try:
            from .crawler.crawler import StoryCrawler
            from .database import Database
            
            crawler = StoryCrawler()
            db = Database()
            
            category_urls = {
                "hot": f"{crawler.settings.base_url}/danh-sach/truyen-hot/",
                "new": f"{crawler.settings.base_url}/danh-sach/truyen-moi/",
                "completed": f"{crawler.settings.base_url}/danh-sach/truyen-full/",
            }
            
            for category in categories:
                if not self.is_running:
                    self._log("â¹ï¸ ÄÃ£ dá»«ng bá»Ÿi ngÆ°á»i dÃ¹ng")
                    break
                    
                if category not in category_urls:
                    continue
                    
                self._log(f"ğŸ“‚ Danh má»¥c: {category}")
                stories = await crawler.crawl_story_list(category_urls[category], max_pages=max_pages)
                self._log(f"  ğŸ“‹ TÃ¬m tháº¥y {len(stories)} truyá»‡n")
                
                for story_info in stories:
                    if not self.is_running:
                        break
                    try:
                        await self._crawl_and_save_story(crawler, db, story_info["source_url"])
                    except Exception as e:
                        self.stats["errors"] += 1
                        continue
            
            self._log(f"ğŸ‰ HoÃ n thÃ nh! {self.stats['stories_crawled']} truyá»‡n, {self.stats['chapters_saved']} chÆ°Æ¡ng")
            return {"status": "completed", "stats": self.stats}
            
        except Exception as e:
            self._log(f"âŒ Lá»—i: {e}")
            return {"status": "failed", "error": str(e)}
        finally:
            self.is_running = False
            self.current_story = ""
    
    def get_status(self):
        """Láº¥y tráº¡ng thÃ¡i scheduler"""
        return {
            "auto_enabled": self.auto_enabled,
            "is_crawling": self.is_running,
            "interval_minutes": self.interval_minutes,
            "last_run": self.last_run.isoformat() if self.last_run else None,
            "current_story": self.current_story,
            "current_story_title": self.current_story_title,
            "progress": self.progress,
            "stats": self.stats,
            "logs": list(self.crawl_logs),
        }

# Singleton instance
scheduler = CrawlScheduler()
